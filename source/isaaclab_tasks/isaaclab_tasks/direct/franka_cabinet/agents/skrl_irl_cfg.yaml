seed: 42


# Models are instantiated using skrl's model instantiator utility
# https://skrl.readthedocs.io/en/latest/api/utils/model_instantiators.html
models:
  separate: True
  policy:  # see gaussian_model parameters
    class: GaussianMixin
    clip_actions: False
    clip_log_std: True
    min_log_std: -20.0
    max_log_std: 2.0
    initial_log_std: 0.0 # 0.0 in normal PPO
    fixed_log_std: False
    network:
      - name: net
        input: STATES
        layers: [256, 128, 64]
        activations: relu
    output: ACTIONS
  value:  # see deterministic_model parameters
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: STATES
        layers: [256, 128, 64]
        activations: relu
    output: ONE
  discriminator:  # see deterministic_model parameters
    class: DeterministicMixin
    clip_actions: False
    network:
      - name: net
        input: STATES_ACTIONS
        layers: [32, 32]
        activations: relu
    output: ONE


# Rollout memory
# https://skrl.readthedocs.io/en/latest/api/memories/random.html
memory:
  class: TrajectoryMemory
  memory_size: -1  # automatically determined (same as agent:rollouts)

motion_dataset:
  class: TrajectoryMemory
  memory_size: 1000000

reply_buffer:
  class: TrajectoryMemory
  memory_size: 102400

# PPO agent configuration (field names are from PPO_DEFAULT_CONFIG)
# https://skrl.readthedocs.io/en/latest/api/agents/ppo.html
agent:
  class: IRL
  rollouts: 100 # Rollout is longer then in PPO (32)
  learning_epochs: 8 # 8 in PPO
  n_discriminator_updates: 15
  mini_batches: 1
  discount_factor: 0.99 # same
  lambda: 0.95
  learning_rate: 5.0e-04 # 5.0e-04 in PPO 5.0e-05 in GAIL
  learning_rate_scheduler: null # KLAdaptiveLR in PPO
  learning_rate_scheduler_kwargs: null
  state_preprocessor: RunningStandardScaler
  state_preprocessor_kwargs: null
  value_preprocessor: RunningStandardScaler
  value_preprocessor_kwargs: null
  amp_state_preprocessor: RunningStandardScaler
  amp_state_preprocessor_kwargs: null
  amp_action_preprocessor: RunningStandardScaler
  amp_action_preprocessor_kwargs: null
  random_timesteps: 0
  learning_starts: 100
  grad_norm_clip: 1.0  # 1.0 in PPO
  ratio_clip: 0.2
  value_clip: 0.2
  clip_predicted_values: True
  entropy_loss_scale: 0.0
  value_loss_scale: 2.0
  discriminator_loss_scale: 1.0 # 5.0
  amp_batch_size: 100
  task_reward_weight: 0.0
  style_reward_weight: 1.0
  discriminator_reward_scale: 1.0
  discriminator_logit_regularization_scale: 0.001
  discriminator_gradient_penalty_scale: 0.01 # 5.0
  discriminator_weight_decay_scale: 1.0e-06
  # rewards_shaper_scale: 1.0
  time_limit_bootstrap: False
  enable_discounted_reward: True
  

  # logging and checkpoint
  experiment:
    directory: "InvertedPendulumGail"
    experiment_name: ""
    write_interval: auto
    checkpoint_interval: auto


# Sequential trainer
# https://skrl.readthedocs.io/en/latest/api/trainers/sequential.html
trainer:
  class: SequentialTrainer
  timesteps: 40000
  environment_info: log

expert_data_path: "logs/skrl/franka_cabinet_direct/memories/25-06-09_18-28-06-884565_memory_0x7f40cf9131c0.npz"